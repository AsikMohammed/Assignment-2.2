Hadoop Distributed File System (HDFS):
      The Hadoop Distributed File System (HDFS) is the primary storage system used by Hadoop applications.HDFS is a distributed file system that provides high-performance access to data across Hadoop clusters. 
      Like other Hadoop-related technologies, HDFS has become a key tool for managing pools of big data and supporting big data analytics applications.
Features of HDFS:
        * It is suitable for the distributed storage and processing. 
        * Unlike other distributed systems, HDFS is highly faulttolerant and designed using low-cost hardware.
        * HDFS holds very large amount of data and provides easier access. 
        * To store such huge data, the files are stored across multiple machines. These files are stored in redundant fashion to rescue the system from possible data losses in case of failure. 
        * HDFS also makes applications available to parallel processing.

Hadoop Clusters:
      A Hadoop cluster is a special type of computational cluster designed specifically for storing and analyzing huge amounts of unstructured data in a distributed computing environment. 
      Hadoop cluster has 3 components:
      1.)Client-It is neither master nor slave, rather play a role of loading the data into cluster, submit MapReduce jobs describing how the data should be processed and then retrieve the data to see the response after job completion. 
      2.)Master-The Masters consists of 3 components NameNode, Secondary Node name and JobTracker. 
      3.)Slave-Slave nodes are the majority of machines in Hadoop Cluster and are responsible to
        a.)Store the data
        b.)Process the computation
NameNode:
        NameNode oversees the health of DataNode and coordinates access to the data stored in DataNode. 
        Name node keeps track of all the file system related information such as to 
        a.) Which section of file is saved in which part of the cluster,
        b.) Last access time for the files
JobTracker:
        JobTracker coordinates the parallel processing of data using MapReduce. 
Secondary Node:
         The job of Secondary Node is to contact NameNode in a periodic manner after certain time interval(by default 1 hour). 
NameNode which keeps all filesystem metadata in RAM has no capability to process that metadata on to disk
         Such clusters run Hadoop's open source distributed processing software on low-cost commodity computers. Hadoop clusters are often referred to as "shared nothing" systems because the only thing that is shared between nodes is the network that connects them. 

HDFS Blocks:
      Hadoop distributed file system also stores the data in terms of blocks. However the block size in HDFS is very large. The default size of HDFS block is 64MB. 
      The files are split into 64MB blocks and then stored into the hadoop filesystem. The hadoop application is responsible for distributing the data blocks across multiple nodes. 
Advantages of HDFS Block:
        * The blocks are of fixed size, so it is very easy to calculate the number of blocks that can be stored on a disk.
        * HDFS block concept simplifies the storage of the datanodes. The datanodes doesnâ€™t need to concern about the blocks metadata data like file permissions etc. 
        *The namenode maintains the metadata of all the blocks.
        * If the size of the file is less than the HDFS block size, then the file does not occupy the complete block storage.
        * As the file is chunked into blocks, it is easy to store a file that is larger than the disk size as the data blocks are distributed and stored on multiple nodes in a hadoop cluster.
